{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "atomic-lotus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "secure-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressBar(value, endvalue, size, bar_length=50):\n",
    "    percent = float(value + 1) / endvalue\n",
    "    arrow = '-' * int(round(percent * bar_length)-1) + '>'\n",
    "    spaces = ' ' * (bar_length - len(arrow))\n",
    "    sys.stdout.write(\"\\r[{0}] {1}/{2} \\t Size : {3}\".format(arrow + spaces, value+1, endvalue, size))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-practice",
   "metadata": {},
   "source": [
    "## Connect URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "exotic-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.theguardian.com/world?page=1'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-arbitration",
   "metadata": {},
   "source": [
    "## Extract Article URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "duplicate-front",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "exact-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_info = bs.findAll('a', {'data-link-name' : 'article'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "happy-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = [article['href'] for article in article_info]\n",
    "article_list = list(set(article_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "derived-vacation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 th Article\n",
      "https://www.theguardian.com/film/2021/oct/11/i-am-belmaya-review-nepali-dalit-filmmaker\n",
      "1 th Article\n",
      "https://www.theguardian.com/commentisfree/2021/oct/16/apples-plan-to-scan-images-will-allow-governments-into-smartphones\n",
      "2 th Article\n",
      "https://www.theguardian.com/world/2021/oct/17/brutal-aggression-venezuela-halts-talks-with-opposition-after-envoy-extradited-to-us\n",
      "3 th Article\n",
      "https://www.theguardian.com/world/2021/oct/17/macron-and-the-french-trump-trap-gaullisms-heirs-in-a-political-vice\n",
      "4 th Article\n",
      "https://www.theguardian.com/world/live/2021/oct/17/covid-news-live-gordon-brown-vaccines-africa-uk-cases-coronavirus-latest\n"
     ]
    }
   ],
   "source": [
    "for i in range(5) :\n",
    "    print('%d th Article' %i)\n",
    "    print(article_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-individual",
   "metadata": {},
   "source": [
    "## Crawling Article Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "collect-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_data(base_url : str, size : int) -> list :\n",
    "    article_data = []\n",
    "    \n",
    "    for i in range(1,size) :\n",
    "        try :\n",
    "            url = base_url + '?page=' + str(i)\n",
    "    \n",
    "            response = requests.get(url)\n",
    "            bs = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            page_url = bs.find('link' , {'rel' : 'canonical'})\n",
    "            page_url = page_url['href']\n",
    "            \n",
    "            if url != page_url :\n",
    "                break\n",
    "    \n",
    "            article_info = bs.findAll('a', {'data-link-name' : 'article'})\n",
    "        \n",
    "            article_list = [article['href'] for article in article_info]\n",
    "            article_list = list(set(article_list))\n",
    "        \n",
    "            article_data.extend(article_list)\n",
    "            progressBar(i, size, len(article_data))\n",
    "        except :\n",
    "            continue\n",
    "            \n",
    "    article_data = list(set(article_data))\n",
    "    return article_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "grave-bottom",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SIZE = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-guyana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>                                                 ] 8/1800 \t Size : 120"
     ]
    }
   ],
   "source": [
    "world_articles = crawl_data('https://www.theguardian.com/world', 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-operator",
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_articles = crawl_data('https://www.theguardian.com/uk-news', 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beneficial-tomorrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------->] 1800/1800 \t Size : 36035"
     ]
    }
   ],
   "source": [
    "tech_articles = crawl_data('https://www.theguardian.com/technology', 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "business_articles = crawl_data('https://www.theguardian.com/business', 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "sport_articles = crawl_data('https://www.theguardian.com/sport', 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-render",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_articles = crawl_data('https://www.theguardian.com/environment', 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-police",
   "metadata": {},
   "outputs": [],
   "source": [
    "culture_articles = crawl_data('https://www.theguardian.com/culture', 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "featured-purpose",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data = world_articles + \\\n",
    "    uk_articles + \\\n",
    "    tech_articles + \\\n",
    "    environment_articles + \\\n",
    "    business_articles + \\\n",
    "    sport_articles + \\\n",
    "    culture_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-simpson",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data = list(set(article_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df = pd.DataFrame({'ID' : range(1, len(article_data)+1), 'URL' : article_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data Size of articles : %d \\n' len(article_df))\n",
    "article_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df.to_csv('theguardinas.com_articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "induced-gross",
   "metadata": {},
   "source": [
    "## Article Data Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "independent-candidate",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArticleCrawler :\n",
    "    \n",
    "    def __init__(self, ) :\n",
    "        pass\n",
    "    \n",
    "    def connect(self, url) :\n",
    "        assert isinstance(url, str)\n",
    "        response = requests.get(url)\n",
    "        bs = BeautifulSoup(response.text, 'html.parser')\n",
    "        return bs\n",
    "    \n",
    "    def get_title(self, bs) :\n",
    "        title_info = bs.title.text\n",
    "        index = title_info.find(' | The Guardian')\n",
    "        return title_info[:index]\n",
    "    \n",
    "    def get_date(self, bs) :\n",
    "        date_info = bs.find('meta', {'property' : 'article:published_time'})\n",
    "        date = date_info['content']\n",
    "        \n",
    "        date_str = date.split('T')[0]\n",
    "        return date_str\n",
    "            \n",
    "    def get_image_url(self, bs) :\n",
    "        url_info = bs.find('meta', {'property' : 'og:image'})\n",
    "        image_url = url_info['content']\n",
    "        return image_url\n",
    "    \n",
    "    def get_image_text(self, bs) :\n",
    "        for tag in bs.findAll('img') :\n",
    "            if 'alt' in tag.attrs :\n",
    "                return tag['alt']\n",
    "            \n",
    "        raise Exception('There is not img alt Attribute')\n",
    "        \n",
    "    def get_text(self, bs) :\n",
    "        main_content = bs.find('div' , {'class' : \"dcr-185kcx9\"})\n",
    "        p_list = main_content.findAll('p')\n",
    "        \n",
    "        text_data = [p.text for p in p_list]\n",
    "        text_data = '\\n'.join(text_data)\n",
    "        return text_data\n",
    "    \n",
    "    def __call__(self, url) :\n",
    "        bs = self.connect(url)\n",
    "        \n",
    "        title = self.get_title(bs)\n",
    "        date = self.get_date(bs)\n",
    "        image_url = self.get_image_url(bs)\n",
    "        image_text = self.get_image_text(bs)\n",
    "        text = self.get_text(bs)\n",
    "        category = url.split('/')[3]\n",
    "        \n",
    "        return {'title' : title, \n",
    "                'date' : date, \n",
    "                'image_url' : image_url,\n",
    "                'image_text' : image_text,\n",
    "                'category' : category,\n",
    "                'text' : text}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-destruction",
   "metadata": {},
   "source": [
    "## Data Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "detailed-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "reported-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Writer :\n",
    "    def __init__(self, dir_path) :\n",
    "        self.dir_path = dir_path\n",
    "        \n",
    "    def __call__(self, article_data : dict) :\n",
    "        assert None not in list(article_data.values())\n",
    "        \n",
    "        title = article_data['title']\n",
    "        date = article_data['date']\n",
    "        category = article_data['category']\n",
    "        image_url = article_data['image_url']\n",
    "        image_text = article_data['image_text']\n",
    "        text = article_data['text']\n",
    "        \n",
    "        image_path = self.save_image(image_url, title)\n",
    "        text_path = self.save_text(text, title)\n",
    "        \n",
    "        return title, date, category, text_path, image_path, image_text\n",
    "        \n",
    "    def save_image(self, image_url : str, title : str) -> str :\n",
    "        image_path = os.path.join(self.dir_path, 'image', title)\n",
    "        image_path = image_path + '.jpg'\n",
    "        \n",
    "        urllib.request.urlretrieve(image_url, image_path)\n",
    "        return image_path\n",
    "        \n",
    "    def save_text(self, text : str, title : str) -> str :\n",
    "        text_path = os.path.join(self.dir_path, 'text', title)\n",
    "        text_path = text_path + '.txt'\n",
    "        \n",
    "        with open(text_path, 'w') as f:\n",
    "            f.write(text)\n",
    "        return text_path\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sticky-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_crawler = ArticleCrawler()\n",
    "writer = Writer('./Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "premium-affair",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(article_crawler, ArticleCrawler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-drink",
   "metadata": {},
   "source": [
    "## Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "integrated-repair",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(article_list) :\n",
    "    title_list = []\n",
    "    date_list = []\n",
    "    cate_list = []\n",
    "    text_paths = []\n",
    "    img_paths = []\n",
    "    img_texts = []\n",
    "\n",
    "    for i, article_url in enumerate(tqdm(article_list)) :\n",
    "        \n",
    "        try :\n",
    "            article_info = article_crawler(article_url)\n",
    "            title, date, category, text_path, image_path, image_text = writer(article_info)\n",
    "        except :\n",
    "            continue\n",
    "    \n",
    "        title_list.append(title)\n",
    "        date_list.append(date)\n",
    "        cate_list.append(category)\n",
    "        text_paths.append(text_path)\n",
    "        img_paths.append(image_path)\n",
    "        img_texts.append(image_text)\n",
    "    \n",
    "    data_df = pd.DataFrame({'title' : title_list, \n",
    "                            'date' : date_list,\n",
    "                            'category' : cate_list, \n",
    "                            'text' : text_paths, \n",
    "                            'image' : img_paths, \n",
    "                            'image_texts' : img_texts})\n",
    "    \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-russell",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
